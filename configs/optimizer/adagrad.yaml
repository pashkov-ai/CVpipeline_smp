_target_: torch.optim.Adagrad
lr: ${general.learning_rate}
r_decay: 0
weight_decay: 0
initial_accumulator_value: 0
eps: 1e-10
foreach: None
maximize: False
differentiable: False
fused: None